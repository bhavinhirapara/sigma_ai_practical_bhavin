{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuGXsrpQB61q"
      },
      "outputs": [],
      "source": [
        "#Extract frames from video\n",
        "\n",
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(\"Video.mp4\")\n",
        "\n",
        "frames = []\n",
        "if cap is not None:\n",
        "  _,frame = cap.read()\n",
        "  frames.append(frame)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rV2guKxYCasi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Motion detection using background substraction\n",
        "import cv2\n",
        "\n",
        "back_sub = cv2.bgsegm.createBackgroundSubtractorMOG();\n",
        "\n",
        "for frame in frames:\n",
        "  fg = back_sub.apply(frame)\n",
        "\n",
        "  contour, _ = cv2.findContours(fg,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  if cv2.contourArea(contour)>500:\n",
        "    x,y,w,h = cv2.boundingRect(contour)\n",
        "    cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),1)\n",
        "  cv2.imshow(\"Motion\", frame)\n",
        "  cv2.waitKey()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "wfHX2JI_CcPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply Segmentation on frame using pytorch model\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load image and predict segmentation using deeplabv3 model\n",
        "def segmentation_frame(image_path):\n",
        "\n",
        "  frame = cv2.imread(image_path)\n",
        "\n",
        "  model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
        "\n",
        "  preprocess = transforms.Compose([transforms.ToPILImage(),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Resize((512,512)),\n",
        "                                   transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])])\n",
        "  input_tensor = preprocess(frame)\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "  output = model(input_batch)['out'][0]\n",
        "  output_pred = output.argmax(0).cpu().numpy()\n",
        "\n",
        "  original_size = (frame.shape[1],frame.shape[0])\n",
        "  resized_segmentation = cv2.resize(output_pred,original_size,interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "  return frame, resized_segmentation\n",
        "\n",
        "# Colors for ovelay\n",
        "def colorize_segmentation(segmentation_map):\n",
        "  #This is array of RGB value to display mask on object\n",
        "  color_map = np.array([0,0,0],\n",
        "                       [128,0,0],\n",
        "                       [0,128,0],\n",
        "                       [0,0,128],\n",
        "                       [128,128,0],\n",
        "                       [0,128,128],\n",
        "                       [128,128,128],\n",
        "                       [64,0,0],\n",
        "                       [0,64,0],\n",
        "                       [0,0,64],\n",
        "                       [64,64,0],\n",
        "                       [0,64,64],\n",
        "                       [128,0,0],\n",
        "                       [0,128,0],\n",
        "                       [0,0,128],\n",
        "                       [64,64,64],\n",
        "                       [128,128,0],\n",
        "                       [0,128,128],\n",
        "                       [95,0,0],\n",
        "                       [0,95,128],)\n",
        "  if np.max(segmentation_map)>=color_map[0]:\n",
        "    raise ValueError(\"Segmentation value error\")\n",
        "\n",
        "  color_segmentation = color_map[segmentation_map]\n",
        "\n",
        "  return color_segmentation\n",
        "\n",
        "# Apply overlay on image\n",
        "def overlay_image(original_image,color_seg,alpha=0.5):\n",
        "  original_float = original_image.astype(float)\n",
        "  color_segmentation_float = color_seg.astype(float)\n",
        "\n",
        "  blend = cv2.addWeighted(original_float,alpha,color_segmentation_float,1-alpha)\n",
        "  return blend.astype('uint8')\n",
        "\n",
        "\n",
        "oringinal_image,resize_seg =segmentation_frame(\"Image.jpg\")\n",
        "color_seg =  colorize_segmentation(resize_seg)\n",
        "blended_image = overlay_image(oringinal_image,color_seg,0.5)\n",
        "\n",
        "cv2_imshow(blended_image)"
      ],
      "metadata": {
        "id": "xYEH3B3fECVx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "16qKrEV-EhAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}